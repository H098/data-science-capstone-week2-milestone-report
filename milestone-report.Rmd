---
title: "Data Science Capstone - Milestone Report"
author: "Gaston Dario Napoli"
date: "May 13, 2017"
output: 
    html_document:
        toc: true
---

```{r setup, include=FALSE}
library(knitr)
library(stringr)
library(tibble)
library(kableExtra)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
options(knitr.table.format = "html") 
```

## 1) Introduction

This document corresponds to the ***Milestone Report***, assignment of week 2, [***Data Science Capstone***](https://www.coursera.org/learn/data-science-project) course from [***Coursera***](https://www.coursera.org/). This course, the 10th out of 10 courses comprising the [***Data Science Specialization***](<https://www.coursera.org/specializations/jhu-data-science>) from the [***John Hopkins University***](https://www.jhu.edu/), allows students to create a usable/public data product that can be used to show your skills to potential employers. Projects will be drawn from real-world problems and will be conducted with industry, government, and academic partners.

The ***Data Science Capstone*** project aims to develop a [***Shiny***](https://shiny.rstudio.com) app that takes as input a phrase (multiple words), one clicks submit, and it will predict the next word. In order to achive that, the [***course dataset***](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) will be used as a training set, as well as NPL techniques will be applied to analyse and build the corresponding predictive model.

## 2) Dataset
### 2.1) Description

From the [course dataset information](https://www.coursera.org/learn/data-science-project/supplement/Iimbd/task-0-understanding-the-problem), the data comes from a corpus called ***HC Corpora*** (the original site is not reachable, but an archive of it can be seen at https://web-beta.archive.org/web/20160930083655/http://www.corpora.heliohost.org/aboutcorpus.html).  ***Corpora*** are collected from publicly available sources by a web crawler.

As said before, the dataset can be downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip, and consists, once being downloaded and uncompressed, of 4 folders corresponding to 4 different languages (german,english, finish and russian), and each folder containing 3 files from 3 different text sources (blogs, news and Twitter):

```{r message=FALSE}
# Get the file list
listOfFiles <- dir("HC_Corpora", recursive = TRUE, full.names = TRUE)

# Show the list as bullets
kable(cbind(
          seq(1, length(listOfFiles)), 
          listOfFiles), 
      col.names = c('#', 'File')) %>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
          full_width = FALSE)
```
Next, a couple of sample lines of Twitter files are shown:

 - ***de_DE.twitter.txt***:
```{r}
connnectionBlogsFile <- file("HC_Corpora/de_DE/de_DE.twitter.txt", "r")
readLines(connnectionBlogsFile, 3)
close(connnectionBlogsFile)
```
 - ***en_US.twitter.txt***:
```{r}
connnectionBlogsFile <- file("HC_Corpora/en_US/en_US.twitter.txt", "r")
readLines(connnectionBlogsFile, 3)
close(connnectionBlogsFile)
```
There exist special characters in the texts, for example question marks. Due to that, they should be taken into account to be discarded in further cleaning data stages.
 
### 2.2) Dataset details
Our analysis starts with a summary table including, for each file in the bundle, file stats (size in bytes) and data derived from the execution of the ***wc*** command (i.e. lines and words counting, and words per line ratio):

```{r message=FALSE}
# Get the file stat list from each file
listOfFileInfos <- data.frame(file = listOfFiles, size = file.info(listOfFiles)$size)
listOfFileInfos$sizeInMB <- round(listOfFileInfos$size / (1024 * 1024), digits = 2)

# Generate four new columns in order to be completed with 'wc' command execution data
listOfFileInfos$lineCount <- 0
listOfFileInfos$wordCount <- 0
listOfFileInfos$wordsPerLineRatio <- 0

# adding a column in order to show the file language
listOfFileInfos <- listOfFileInfos %>%
  rowwise() %>% 
  mutate(language = 
           ifelse(str_detect(file, "en_US"), 'English', 
             ifelse(str_detect(file, "de_DE"), 'German',
               ifelse(str_detect(file, "fi_FI"), 'Finnish',
                 ifelse(str_detect(file, "ru_RU"), 'Russian', 'not-defined')))))

# Auxiliary function. It allows get data from files using the 'wc' command.
executeWc <- function(x) as.numeric(str_split(system(paste0("wc ", x), intern = TRUE),  boundary("word"))[[1]][1:2])

# Complete de file stats with the 'wc' command data
for (index in 1:nrow(listOfFileInfos)) {
  wcCommandResults <- executeWc(listOfFileInfos[index,]$file)
  
  listOfFileInfos[index,]$lineCount <- wcCommandResults[1]
  listOfFileInfos[index,]$wordCount <- wcCommandResults[2]
  listOfFileInfos[index,]$wordsPerLineRatio <- round(wcCommandResults[2] / wcCommandResults[1], digits = 2)
}

columNamesToShow <- c('File', 'Size', 'Size in MB', 'Line count', 'Word count', 'W/L ratio', 'Language')

# Show a formatted table
kable(listOfFileInfos, col.names = columNamesToShow)  %>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
          full_width = FALSE)

```

In the context of the Capstone project, only the english language files will be taken into account, that is:

```{r results='asis'}
# Select files in english language
englishFiles <- listOfFileInfos[listOfFileInfos$language == "English",]

# Show a formatted table
kable(englishFiles, col.names = columNamesToShow)%>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
          full_width = FALSE)

```




## Apendix I - Source codes

This document has been generated using [***R Mardown***](http://rmarkdown.rstudio.com/). Its ***.Rmd*** source code that can be found at: <https://github.com/laplata2003/data-science-capstone-week2-milestone-report>.

